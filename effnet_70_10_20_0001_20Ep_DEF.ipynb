{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "048ee6ce",
      "metadata": {
        "id": "048ee6ce"
      },
      "source": [
        "## Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3928c2",
      "metadata": {
        "id": "4a3928c2"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = './miocarditeDataset'\n",
        "NORMAL_DIR = './miocarditeDataset/Normal'\n",
        "ANORMAL_DIR = './miocarditeDataset/Abnormal'\n",
        "SICK_DIR = './miocarditeDataset/Sick/*/*'\n",
        "SICK_DIR_DS = './miocarditeDataset/Sick'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee510ec",
      "metadata": {
        "id": "9ee510ec"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b607c61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b607c61",
        "outputId": "8fa2da79-8165-4ed1-e121-a681a61b5578"
      },
      "outputs": [],
      "source": [
        "pathImgNormal = glob(os.path.join(NORMAL_DIR,'*.jpg'))\n",
        "pathImgAnormal = glob(os.path.join(ANORMAL_DIR,'*.jpg'))\n",
        "pathImgSick = glob(os.path.join(SICK_DIR,'*.jpg'))\n",
        "print('Numero de imgs normais: {}'.format(len(pathImgNormal)))\n",
        "print('Numero de imgs com miocardite: {}'.format(len(pathImgAnormal)))\n",
        "print('Numero de imgs doentes: {}'.format(len(pathImgSick)))\n",
        "pathImgData = pathImgNormal + pathImgAnormal + pathImgSick\n",
        "print('Total de registros: {}'.format(len(pathImgData)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd79719",
      "metadata": {
        "id": "3fd79719",
        "outputId": "be3c5dba-c84c-48e6-dc70-4d604becd2c5"
      },
      "outputs": [],
      "source": [
        "!pip install kmeans-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "152b0b91",
      "metadata": {
        "id": "152b0b91"
      },
      "outputs": [],
      "source": [
        "## K-Means\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from kmeans_pytorch import kmeans, kmeans_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb85d44",
      "metadata": {
        "id": "9bb85d44",
        "outputId": "08c49fea-3d8e-4b1d-f316-0a080aed1668"
      },
      "outputs": [],
      "source": [
        "prep_transform = T.Compose([\n",
        "                  T.ToTensor(),\n",
        "                  T.Resize(size=(100,100)),\n",
        "                ])\n",
        "\n",
        "images_normal = []\n",
        "images_anormal = []\n",
        "\n",
        "for img in pathImgNormal:\n",
        "    image = Image.open(img)\n",
        "    image = prep_transform(image)\n",
        "    image = torch.flatten(image,1)\n",
        "    images_normal.append(image.numpy())\n",
        "\n",
        "for img in pathImgAnormal:\n",
        "    image = Image.open(img)\n",
        "    image = prep_transform(image)\n",
        "    image = torch.flatten(image,1)\n",
        "    images_anormal.append(image.numpy())\n",
        "\n",
        "images_normal = torch.tensor(np.array(images_normal))\n",
        "images_anormal = torch.tensor(np.array(images_anormal))\n",
        "\n",
        "print(images_normal.shape)\n",
        "print(images_anormal.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92de5f4c",
      "metadata": {
        "id": "92de5f4c",
        "outputId": "2bb7b7c9-841f-468c-fca9-115b0a0cfef7"
      },
      "outputs": [],
      "source": [
        "data_size, dims, num_clusters = 2449, 10000, 2\n",
        "data_size2 = 4686\n",
        "\n",
        "cluster_ids_x, cluster_centers = kmeans(\n",
        "    X=images_normal, num_clusters=num_clusters, distance='euclidean',device=torch.device('cuda:0')\n",
        ")\n",
        "\n",
        "cluster_ids_x2, cluster_centers2 = kmeans(\n",
        "    X=images_anormal, num_clusters=num_clusters, distance='euclidean',device=torch.device('cuda:0')\n",
        ")\n",
        "\n",
        "# cluster IDs and cluster centers normal\n",
        "print(cluster_ids_x)\n",
        "print(cluster_centers)\n",
        "\n",
        "# cluster IDs and cluster centers anormal\n",
        "print(cluster_ids_x2)\n",
        "print(cluster_centers2)\n",
        "\n",
        "print(np.count_nonzero(cluster_ids_x == 0))\n",
        "print(np.count_nonzero(cluster_ids_x == 1))\n",
        "\n",
        "print(np.count_nonzero(cluster_ids_x2 == 0))\n",
        "print(np.count_nonzero(cluster_ids_x2 == 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5961dcda",
      "metadata": {
        "id": "5961dcda"
      },
      "outputs": [],
      "source": [
        "NORMAL0_DIR = DATA_DIR + \"/normal0\"\n",
        "NORMAL1_DIR = DATA_DIR + \"/normal1\"\n",
        "ANORMAL2_DIR = DATA_DIR + \"/anormal2\"\n",
        "ANORMAL3_DIR = DATA_DIR + \"/anormal3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45b647a2",
      "metadata": {
        "id": "45b647a2"
      },
      "outputs": [],
      "source": [
        "# Limpando os diretorios\n",
        "filelist = glob(os.path.join(NORMAL0_DIR,\"*\"))\n",
        "for f in filelist:\n",
        "    os.remove(f)\n",
        "filelist = glob(os.path.join(NORMAL1_DIR,\"*\"))\n",
        "for f in filelist:\n",
        "    os.remove(f)\n",
        "filelist = glob(os.path.join(ANORMAL2_DIR,\"*\"))\n",
        "for f in filelist:\n",
        "    os.remove(f)\n",
        "filelist = glob(os.path.join(ANORMAL3_DIR,\"*\"))\n",
        "for f in filelist:\n",
        "    os.remove(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b0a98ef",
      "metadata": {
        "id": "5b0a98ef",
        "outputId": "25115802-bf3b-4e8a-db63-d59bea4a6248"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "for i in range(len(cluster_ids_x)):\n",
        "    if(cluster_ids_x[i] == 0):\n",
        "        shutil.copy2(pathImgNormal[i],NORMAL0_DIR)\n",
        "    else:\n",
        "        shutil.copy2(pathImgNormal[i],NORMAL1_DIR)\n",
        "\n",
        "print(len(os.listdir(NORMAL0_DIR)))\n",
        "print(len(os.listdir(NORMAL1_DIR)))\n",
        "\n",
        "for i in range(len(cluster_ids_x2)):\n",
        "    if(cluster_ids_x2[i] == 0):\n",
        "        shutil.copy2(pathImgAnormal[i],ANORMAL2_DIR)\n",
        "    else:\n",
        "        shutil.copy2(pathImgAnormal[i],ANORMAL3_DIR)\n",
        "\n",
        "print(len(os.listdir(ANORMAL2_DIR)))\n",
        "print(len(os.listdir(ANORMAL3_DIR)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec3bd0f6",
      "metadata": {
        "id": "ec3bd0f6",
        "outputId": "164a6858-3d22-40f8-bf29-2f6552deafb2"
      },
      "outputs": [],
      "source": [
        "## Criando CSV\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "images2 = []\n",
        "targets2 = []\n",
        "\n",
        "path_normal0 = glob(os.path.join(NORMAL0_DIR,'*.jpg'))\n",
        "path_normal1 = glob(os.path.join(NORMAL1_DIR,'*.jpg'))\n",
        "path_anormal2 = glob(os.path.join(ANORMAL2_DIR,'*.jpg'))\n",
        "path_anormal3 = glob(os.path.join(ANORMAL3_DIR,'*.jpg'))\n",
        "\n",
        "\n",
        "for img in path_normal0:\n",
        "  images2.append(img.split('/')[-1])\n",
        "  targets2.append(0)\n",
        "\n",
        "for img in path_normal1:\n",
        "  images2.append(img.split('/')[-1])\n",
        "  targets2.append(1)\n",
        "\n",
        "for img in path_anormal2:\n",
        "  images2.append(img.split('/')[-1])\n",
        "  targets2.append(2)\n",
        "\n",
        "for img in path_anormal3:\n",
        "  images2.append(img.split('/')[-1])\n",
        "  targets2.append(3)\n",
        "\n",
        "for img in pathImgSick:\n",
        "  aux = img.split('/')[2:]\n",
        "  images2.append(aux[1] + '/' + aux[2] + '/' + aux[3]) #\n",
        "  targets2.append(4)\n",
        "\n",
        "# [imagem.jpg, 0] -> 0,1: coração normal | 2,3: coração com miocardite | 4: coração doente\n",
        "\n",
        "info = {\"image\": images2, \"target\": targets2}\n",
        "\n",
        "# Embaralhando o registro\n",
        "\n",
        "df = pd.DataFrame(info)\n",
        "df = df.sample(frac = 1)\n",
        "\n",
        "df.to_csv('data.csv', index = False)\n",
        "\n",
        "\n",
        "data = pd.read_csv('./data.csv')\n",
        "# data.target\n",
        "data.describe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be6a424d",
      "metadata": {
        "id": "be6a424d"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9123c56d",
      "metadata": {
        "id": "9123c56d"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch import nn, optim\n",
        "import time\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8b5daa",
      "metadata": {
        "id": "9e8b5daa"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = './miocarditeDataset'\n",
        "NORMAL_DIR = './miocarditeDataset/Normal'\n",
        "ANORMAL_DIR = './miocarditeDataset/Abnormal'\n",
        "NORMAL0_DIR = DATA_DIR + \"/normal0\"\n",
        "NORMAL1_DIR = DATA_DIR + \"/normal1\"\n",
        "ANORMAL2_DIR = DATA_DIR + \"/anormal2\"\n",
        "ANORMAL3_DIR = DATA_DIR + \"/anormal3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd6aeb3c",
      "metadata": {
        "id": "fd6aeb3c",
        "outputId": "b69fd199-ddb7-4229-98a4-47f70f01114d"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('./data.csv')\n",
        "data.describe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193d1657",
      "metadata": {
        "id": "193d1657",
        "outputId": "ab09a162-821a-46f4-c6f9-cbc0a881e81a"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "\n",
        "  epochs =20                              # No. of epochs for training the model\n",
        "  lr = 0.001                              # Learning rate\n",
        "  batch_size = 16                         # Batch Size for Dataset\n",
        "\n",
        "  model_name = 'tf_efficientnet_b4_ns'    # Model name (we are going to import model from timm)\n",
        "  img_size = 224                          # Resize all the images to be 224 by 224\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"On which device we are on:{}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cda4bbf",
      "metadata": {
        "id": "1cda4bbf"
      },
      "outputs": [],
      "source": [
        "class MiocarditeDataset(Dataset):\n",
        "\n",
        "  def __init__(self, csv_file, root_dir, transform = None):\n",
        "    self.miocardite_frame = pd.read_csv('./data.csv')\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.miocardite_frame)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    target = self.miocardite_frame.iloc[idx,1]\n",
        "\n",
        "    if target == 0:\n",
        "        dir = NORMAL0_DIR\n",
        "    elif target == 1:\n",
        "        dir = NORMAL1_DIR\n",
        "    elif target == 2:\n",
        "        dir = ANORMAL2_DIR\n",
        "    elif target == 3:\n",
        "        dir = ANORMAL3_DIR\n",
        "    else:\n",
        "        dir = SICK_DIR_DS\n",
        "\n",
        "\n",
        "    img_name = os.path.join(dir, self.miocardite_frame.iloc[idx,0])\n",
        "    image = Image.open(img_name)\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    sample = (image, target)\n",
        "\n",
        "    return sample\n",
        "\n",
        "prep_transform = T.Compose([\n",
        "                    T.Grayscale(num_output_channels=3),\n",
        "                    T.ToTensor(),\n",
        "                    T.Resize(size=(CFG.img_size,CFG.img_size),antialias=None),\n",
        "                    T.CenterCrop(size=224),\n",
        "                  ])\n",
        "\n",
        "miocardite_dataset = MiocarditeDataset(csv_file = './data.csv',\n",
        "                          root_dir = DATA_DIR,\n",
        "                          transform = prep_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21bca04",
      "metadata": {
        "id": "c21bca04",
        "outputId": "fc1aa240-4b01-452b-c678-f5b95ad84384"
      },
      "outputs": [],
      "source": [
        "img = './410.jpg'\n",
        "img = Image.open(img)\n",
        "prep_transform = T.Compose([\n",
        "                    T.Grayscale(num_output_channels=3),\n",
        "                    T.Resize(size=(CFG.img_size,CFG.img_size),antialias=None),\n",
        "                    T.CenterCrop(size=224),\n",
        "                  ])\n",
        "img = prep_transform(img)\n",
        "\n",
        "plt.imshow(img,cmap='gray')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3f62e39",
      "metadata": {
        "id": "e3f62e39",
        "outputId": "89ce33bc-a6df-46c9-84c8-50f768f50758"
      },
      "outputs": [],
      "source": [
        "num_classes = 4\n",
        "\n",
        "train_ds, valid_ds, test_ds = random_split(miocardite_dataset,[0.7,0.1,0.2])\n",
        "\n",
        "valid_data_size = len(valid_ds)\n",
        "train_data_size = len(train_ds)\n",
        "\n",
        "train_data = DataLoader(train_ds, batch_size = CFG.batch_size, shuffle = True)\n",
        "valid_data = DataLoader(valid_ds, batch_size = CFG.batch_size, shuffle = True)\n",
        "test_data = DataLoader(test_ds, batch_size = CFG.batch_size, shuffle = True)\n",
        "\n",
        "print(\"Amostras p/ treinamento: \",len(train_ds))\n",
        "print(\"Amostras p/ validação: \",len(valid_ds))\n",
        "print(\"Amostras p/ teste: \",len(test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89759ce8",
      "metadata": {
        "id": "89759ce8"
      },
      "outputs": [],
      "source": [
        "test_images = []\n",
        "test_targets = []\n",
        "for test in test_ds:\n",
        "    image, target = test\n",
        "    test_images.append(image)\n",
        "    test_targets.append(target)\n",
        "\n",
        "test_csv = {\n",
        "    \"image\": test_images,\n",
        "    \"target\": test_targets\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(test_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d69b831",
      "metadata": {
        "id": "3d69b831"
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "677f5095",
      "metadata": {
        "id": "677f5095",
        "outputId": "a5d62068-e132-43dd-9161-1f0173b2e700"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm # PyTorch Image Models\n",
        "\n",
        "model = timm.create_model(CFG.model_name,pretrained=True) #load pretrained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f57479",
      "metadata": {
        "id": "95f57479"
      },
      "outputs": [],
      "source": [
        "#let's update the pretarined model:\n",
        "for param in model.parameters():\n",
        "  param.requires_grad=False\n",
        "\n",
        "\n",
        "#we are updating it as a 2-class classifier:\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(in_features=1792, out_features=625), #1792 is the orginal in_features\n",
        "    nn.ReLU(), #ReLu to be the activation function\n",
        "    nn.Dropout(p=0.3),\n",
        "    nn.Linear(in_features=625, out_features=256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=256, out_features=5),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf87d56",
      "metadata": {
        "id": "1cf87d56"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "loss_func = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=CFG.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df2c19ed",
      "metadata": {
        "id": "df2c19ed"
      },
      "outputs": [],
      "source": [
        "def train(model, train_data, loss_func, optimizer):\n",
        "    # Set to training mode\n",
        "    model.train()\n",
        "    # Loss and Accuracy within the epoch\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    train_cumloss = 0.0\n",
        "    train_cumacc = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_data):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Clean existing gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass - compute outputs on input data using the model\n",
        "        outputs = model(inputs)\n",
        "        # Compute loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        # Backpropagate the gradients\n",
        "        loss.backward()\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "        # Compute the total loss for the batch and add it to train_loss\n",
        "        train_cumloss += loss.item()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        # Compute the accuracy\n",
        "        ret, predictions = torch.max(outputs.data, 1)\n",
        "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "        # Convert correct_counts to float and then compute the mean\n",
        "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "        # Compute total accuracy in the whole batch and add to train_acc\n",
        "        train_cumacc += acc.item()\n",
        "        train_acc += acc.item() * inputs.size(0)\n",
        "#         print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "    return (train_loss, train_acc, train_cumloss, train_cumacc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d075fb",
      "metadata": {
        "id": "82d075fb"
      },
      "outputs": [],
      "source": [
        "def validade(model, valid_data, loss_func):\n",
        "    # Set to evaluation mode\n",
        "    model.eval()\n",
        "    # Loss and Accuracy within the epoch\n",
        "    valid_loss = 0.0\n",
        "    valid_acc = 0.0\n",
        "    valid_cumloss = 0.0\n",
        "    valid_cumacc = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Validation loop\n",
        "        for j, (inputs, labels) in enumerate(valid_data):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            # Compute the total loss for the batch and add it to valid_loss\n",
        "            valid_cumloss += loss.item()\n",
        "            valid_loss += loss.item() * inputs.size(0)\n",
        "            # Calculate validation accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            # Compute total accuracy in the whole batch and add to valid_acc\n",
        "            valid_cumacc += acc.item()\n",
        "            valid_acc += acc.item() * inputs.size(0)\n",
        "#             print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "\n",
        "    return(valid_loss, valid_acc, valid_cumloss, valid_cumacc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99023c22",
      "metadata": {
        "id": "99023c22",
        "outputId": "5f62b1cd-56bb-4603-9e6b-a6c1fba34388"
      },
      "outputs": [],
      "source": [
        "epochs = CFG.epochs\n",
        "\n",
        "loss_criterion = nn.NLLLoss()\n",
        "\n",
        "conv_train_losses = []\n",
        "conv_test_losses = []\n",
        "conv_train_acc = []\n",
        "conv_test_acc = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "\n",
        "        train_loss, train_acc, train_cumloss, train_cumacc = train(model, train_data, loss_func, optimizer)\n",
        "        conv_train_losses.append(train_cumloss/len(train_data))\n",
        "        conv_train_acc.append(train_cumacc/len(train_data))\n",
        "\n",
        "        valid_loss, valid_acc, valid_cumloss, valid_cumacc = validade(model, valid_data, loss_func)\n",
        "        conv_test_losses.append(valid_cumloss/len(valid_data))\n",
        "        conv_test_acc.append(valid_cumacc/len(valid_data))\n",
        "\n",
        "avg_train_loss = train_loss/train_data_size\n",
        "avg_train_acc = train_acc/float(train_data_size)\n",
        "# Find average training loss and training accuracy\n",
        "avg_valid_loss = valid_loss/valid_data_size\n",
        "avg_valid_acc = valid_acc/float(valid_data_size)\n",
        "history = ([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
        "epoch_end = time.time()\n",
        "print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, nttValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44104f86",
      "metadata": {
        "id": "44104f86"
      },
      "source": [
        "## Carregando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6901e15e",
      "metadata": {
        "id": "6901e15e"
      },
      "outputs": [],
      "source": [
        "def plot_losses(losses):\n",
        "    fig = plt.figure(figsize=(13, 5))\n",
        "    ax = fig.gca()\n",
        "    for loss_name, loss_values in losses.items():\n",
        "        ax.plot(loss_values, label=loss_name)\n",
        "    ax.legend(fontsize=\"16\")\n",
        "    ax.set_xlabel(\"Iteration\", fontsize=\"16\")\n",
        "    ax.set_ylabel(\"Loss\", fontsize=\"16\")\n",
        "    ax.set_title(\"Loss vs iterations\", fontsize=\"16\")\n",
        "    plt.savefig('./graficos/70-10-20_20eph_0001LR_Loss_ART.png', format='png')\n",
        "\n",
        "def plot_acc(acc):\n",
        "    fig = plt.figure(figsize=(13, 5))\n",
        "    ax = fig.gca()\n",
        "    for acc_name, acc_values in acc.items():\n",
        "        ax.plot(acc_values, label=acc_name)\n",
        "    ax.legend(fontsize=\"16\")\n",
        "    ax.set_xlabel(\"Epochs\", fontsize=\"16\")\n",
        "    ax.set_ylabel(\"Accuracy\", fontsize=\"16\")\n",
        "    ax.set_title(\"Accuracy vs iterations\", fontsize=\"16\")\n",
        "    plt.savefig('./graficos/70-10-20_20eph_0001LR_Acc_ART.png', format='png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65439723",
      "metadata": {
        "id": "65439723",
        "outputId": "906326ff-4e8c-4b8f-eb22-67e1a46b03be"
      },
      "outputs": [],
      "source": [
        "conv_losses = {\"Train Loss\": conv_train_losses, \"Test Loss\": conv_test_losses}\n",
        "plot_losses(conv_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb99b93",
      "metadata": {
        "id": "0fb99b93",
        "outputId": "e52c3cc3-ac3d-4ca9-ce1a-e33d6363a505"
      },
      "outputs": [],
      "source": [
        "conv_acc = {\"Train Acc\": conv_train_acc, \"Test Acc\": conv_test_acc}\n",
        "plot_acc(conv_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44900fa3",
      "metadata": {
        "id": "44900fa3"
      },
      "outputs": [],
      "source": [
        "def predict(model, teste):\n",
        "    img, target = teste\n",
        "    test_image = transf(img)\n",
        "#     plt.imshow(test_image)\n",
        "    test_image_tensor = img\n",
        "    if torch.cuda.is_available():\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
        "    else:\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Model outputs log probabilities\n",
        "        out = model(test_image_tensor)\n",
        "        ps = torch.exp(out)\n",
        "#         topk, topclass = ps.topk(1, dim=1)\n",
        "        class_prob = torch.softmax(out, dim=1)\n",
        "        # get most probable class and its probability:\n",
        "        class_prob, topclass1 = torch.max(class_prob, dim=1)\n",
        "        print(\"Out: \",out)\n",
        "        print(\"Predict: \",topclass1)\n",
        "        print(\"Prob: \",class_prob)\n",
        "        print(\"Target: \",target)\n",
        "        print(\"======================\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8e49ea0",
      "metadata": {
        "id": "e8e49ea0"
      },
      "outputs": [],
      "source": [
        "# Salvando modelo\n",
        "torch.save(model.state_dict(), 'model/efficientnet_model_weights_70_10_20_0001_ART.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a53968",
      "metadata": {
        "id": "18a53968"
      },
      "outputs": [],
      "source": [
        "def make_confusion_matrix(model, loader, n_classes):\n",
        "  confusion_matrix = torch.zeros(n_classes, n_classes, dtype=torch.int64)\n",
        "  with torch.no_grad():\n",
        "    for i, (imgs, target) in enumerate(loader):\n",
        "      imgs = imgs.to(device)\n",
        "      target = target.to(device)\n",
        "      outputs = model(imgs)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      for t, p in zip(torch.as_tensor(target, dtype=torch.int64).view(-1),\n",
        "                      torch.as_tensor(predicted, dtype=torch.int64).view(-1)):\n",
        "        confusion_matrix[t, p] += 1\n",
        "  return confusion_matrix\n",
        "\n",
        "def evaluate_accuracy(model, dataloader, classes, verbose=True):\n",
        "  # prepare to count predictions for each class\n",
        "  correct_pred = {classname: 0 for classname in classes}\n",
        "  total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "  confusion_matrix = make_confusion_matrix(model, dataloader, len(classes))\n",
        "  if verbose:\n",
        "    total_correct = 0.0\n",
        "    total_prediction = 0.0\n",
        "    for i, classname in enumerate(classes):\n",
        "      correct_count = confusion_matrix[i][i].item()\n",
        "      class_pred = torch.sum(confusion_matrix[i]).item()\n",
        "\n",
        "      total_correct += correct_count\n",
        "      total_prediction += class_pred\n",
        "\n",
        "      accuracy = 100 * float(correct_count) / class_pred\n",
        "      print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
        "                                                    accuracy))\n",
        "  print(\"Global acccuracy is {:.1f}\".format(100 * total_correct/total_prediction))\n",
        "  return confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc7072c",
      "metadata": {
        "id": "7dc7072c",
        "outputId": "889c0c68-a16a-4d36-bb31-7b63f5eac0b2"
      },
      "outputs": [],
      "source": [
        "CATEGORIES = ['normal0','normal1','anormal2','anormal3','sick']\n",
        "\n",
        "conv_confusion_matrix = evaluate_accuracy(model, valid_data, CATEGORIES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "374d5b55",
      "metadata": {
        "id": "374d5b55"
      },
      "outputs": [],
      "source": [
        "def show_CMR(image):\n",
        "    transf = T.ToPILImage()\n",
        "    image = transf(image)\n",
        "    plt.imshow(image,cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88177ca6",
      "metadata": {
        "id": "88177ca6"
      },
      "source": [
        "## Testes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee68f4cd",
      "metadata": {
        "id": "ee68f4cd",
        "outputId": "ddd63dc3-6bb9-454f-c5e4-44ae1d464dde"
      },
      "outputs": [],
      "source": [
        "!pip install captum\n",
        "!pip uninstall matplotlib -y\n",
        "!pip install matplotlib==3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572a3c78",
      "metadata": {
        "id": "572a3c78"
      },
      "outputs": [],
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "from captum.attr import GradientShap\n",
        "from captum.attr import Occlusion\n",
        "from captum.attr import NoiseTunnel\n",
        "from captum.attr import visualization as viz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c14ede7",
      "metadata": {
        "id": "1c14ede7",
        "outputId": "a0b04b27-a630-4388-97ac-6e3d6d33c430"
      },
      "outputs": [],
      "source": [
        "conv_confusion_matrix = evaluate_accuracy(model, test_data, CATEGORIES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87dd850a",
      "metadata": {
        "id": "87dd850a",
        "outputId": "29e20069-2eeb-4385-9348-ba3f04c8eaea"
      },
      "outputs": [],
      "source": [
        "image_test, target_test = test_ds[0]\n",
        "\n",
        "print(image_test.shape)\n",
        "if torch.cuda.is_available():\n",
        "    input = image_test.view(1, 3, 224, 224).cuda()\n",
        "else:\n",
        "    input = image_test.view(1, 3, 224, 224)\n",
        "\n",
        "output = model(input)\n",
        "output = F.softmax(output, dim=1)\n",
        "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
        "\n",
        "print('Predicted:', pred_label_idx, '(', prediction_score.squeeze().item(), ')')\n",
        "print('Target: ',target_test)\n",
        "\n",
        "occlusion = Occlusion(model)\n",
        "\n",
        "attributions_occ = occlusion.attribute(input,\n",
        "                                       strides = (3, 8, 8),\n",
        "                                       target=pred_label_idx,\n",
        "                                       sliding_window_shapes=(3,15, 15),\n",
        "                                       baselines=(1, 1, 1))\n",
        "\n",
        "figure, ax = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      np.transpose(image_test.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      [\"blended_heat_map\"],\n",
        "                                      [\"all\"],\n",
        "                                      show_colorbar=True,\n",
        "                                      outlier_perc=2,\n",
        "                                      fig_size=(3, 3)\n",
        "                                     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3fd749",
      "metadata": {
        "id": "cc3fd749",
        "outputId": "a4f31038-1173-4dc6-e1a7-de5ef7069363"
      },
      "outputs": [],
      "source": [
        "TESTS_OCC_DIR = './testesOcclusion/*'\n",
        "pathImgTest = glob(os.path.join(TESTS_OCC_DIR,'*.jpg'))\n",
        "\n",
        "prep_transform = T.Compose([\n",
        "                    T.Grayscale(num_output_channels=3),\n",
        "                    T.ToTensor(),\n",
        "                    T.Resize(size=(CFG.img_size,CFG.img_size),antialias=None),\n",
        "                    T.CenterCrop(size=224),\n",
        "                  ])\n",
        "\n",
        "for img in pathImgTest:\n",
        "    image = Image.open(img)\n",
        "    image = prep_transform(image)\n",
        "    img_tensor = image\n",
        "\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        image = image.view(1, 3, 224, 224).cuda()\n",
        "    else:\n",
        "        image = image.view(1, 3, 224, 224)\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        out = model(image)\n",
        "        ps = torch.exp(out)\n",
        "        class_prob = torch.softmax(out, dim=1)\n",
        "        class_prob, topclass1 = torch.max(class_prob, dim=1)\n",
        "\n",
        "        print(\"Imagem: \",img)\n",
        "        print(\"Out: \",out)\n",
        "        print(\"Predict: \",topclass1)\n",
        "        print(\"Prob: \",class_prob)\n",
        "        print(\"======================\\n\")\n",
        "\n",
        "        occlusion = Occlusion(model)\n",
        "\n",
        "        attributions_occ = occlusion.attribute(input,\n",
        "                                               strides = (3, 8, 8),\n",
        "                                               target=topclass1,\n",
        "                                               sliding_window_shapes=(3,15, 15),\n",
        "                                               baselines=(0,0,0))\n",
        "\n",
        "        _ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                              np.transpose(img_tensor.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                              [\"blended_heat_map\"],\n",
        "                                              [\"all\"],\n",
        "                                              show_colorbar=True,\n",
        "                                              outlier_perc=2,\n",
        "                                             )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2f5a18c",
      "metadata": {
        "id": "c2f5a18c",
        "outputId": "e46ff8c7-7fa3-47b2-f261-f26ec55c7ffb"
      },
      "outputs": [],
      "source": [
        "err = 0\n",
        "\n",
        "transform_normalize = T.Normalize(\n",
        "     mean=[0.485, 0.456, 0.406],\n",
        "     std=[0.229, 0.224, 0.225]\n",
        " )\n",
        "\n",
        "\n",
        "for test in test_ds:\n",
        "    img, target = test\n",
        "    image = img\n",
        "    if torch.cuda.is_available():\n",
        "        image = img.view(1, 3, 224, 224).cuda()\n",
        "    else:\n",
        "        image = img.view(1, 3, 224, 224)\n",
        "\n",
        "    input = transform_normalize(image)\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Model outputs log probabilities\n",
        "        out = model(image)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ps = torch.exp(out)\n",
        "    #         topk, topclass = ps.topk(1, dim=1)\n",
        "        class_prob = torch.softmax(out, dim=1)\n",
        "        # get most probable class and its probability:\n",
        "        class_prob, topclass1 = torch.max(class_prob, dim=1)\n",
        "        if topclass1 != target:\n",
        "            print(\"ERRO\")\n",
        "            err+=1\n",
        "\n",
        "        print(\"Out: \",out)\n",
        "        print(\"Predict: \",topclass1)\n",
        "        print(\"Target: \", target)\n",
        "        print(\"Prob: \",class_prob)\n",
        "        print(\"======================\\n\")\n",
        "\n",
        "        occlusion = Occlusion(model)\n",
        "\n",
        "        attributions_occ = occlusion.attribute(input,\n",
        "                                               strides = (3, 8, 8),\n",
        "                                               target=topclass1,\n",
        "                                               sliding_window_shapes=(3,15, 15),\n",
        "                                               baselines=(0,0,0))\n",
        "\n",
        "        _ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                              np.transpose(img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                              [\"blended_heat_map\"],\n",
        "                                              [\"all\"],\n",
        "                                              show_colorbar=True,\n",
        "                                              outlier_perc=2,\n",
        "                                             )\n",
        "\n",
        "print(\"Result: \",1 - err/len(test_ds))\n",
        "print(\"Quantidade de amostras analisadas: \",len(test_ds))\n",
        "print(\"Erros: \",err)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
